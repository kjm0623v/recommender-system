{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4장. 추천 엔진에서 사용되는 데이터 마이닝 기법\n",
    "\n",
    "이번 장에서는 주로 데이터를 다루기 위한 테크닉을 다룰 것이다. 유사도 측정, 머신러닝부터 평가 방법까지 다양하게 다룬다.\n",
    "\n",
    "1. 이웃 기반 기법\n",
    "    - 유클리드 거리(Euclidean distance)\n",
    "    - 코사인 유사도(Cosine similarity)\n",
    "    - 자카드 유사도(Jaccard similarity)\n",
    "    - 피어슨 상관계수(Pearson correlation coefficient) <br><br>\n",
    "2. 수학적 모델링 기법 \n",
    "    - 행렬 인수 분해(NMF)\n",
    "    - 교대 최소 제곱(ALS)\n",
    "    - 특이값 분해(SVD) <br><br>\n",
    "3. 머신러닝 기법\n",
    "    - 선형 회귀(Logistic Regression)\n",
    "    - 분류 모델(Classification) <br><br>\n",
    "4. 클러스터링 기법\n",
    "    - K-mean 클러스터링 <br><br>\n",
    "5. 차원 축소\n",
    "    - 주성분 분석(PCA) <br><br>\n",
    "6. 벡터 공간 모델 \n",
    "    - 단어 빈도\n",
    "    - 단어 빈도-역문서 빈도 <br><br>\n",
    "7. 평가 기법\n",
    "    - 평균 제곱근 오차(RMSE)\n",
    "    - 평균 절대 오차(MAE)\n",
    "    - 정밀도(Precision) 와 재현율(Recall) <br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 민코우스키 거리\n",
    "\n",
    "맨하튼 거리, 유클리드 거리 등의 기본이 되는 거리를 측정하는 식이다. 유클리드 거리, 맨하튼 거리의 일반화된 식이라고 보면 된다.\n",
    "\n",
    "$\\Bigg ( \\sqrt{\\displaystyle \\sum_{i=1}^n \\lvert x_i - y_i \\rvert^p} \\Bigg ) ^{\\frac 1 p}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유클리드 거리 \n",
    "\n",
    "$Euclidean\\ Distance(x, y) =  \\sqrt{\\displaystyle \\sum_{i=1}^n \\lvert x_i - y_i \\rvert^2}$ <br>\n",
    "*($x_i, y_i$는 같은 차원에 매칭되는 점이며 $n$은 데이터의 총 차원 수를 나타낸다.)* <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "x = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], ndmin=2)\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0], ndmin=2)\n",
    "\n",
    "assert euclidean(x, y) == norm(x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코사인 유사도 \n",
    "\n",
    "$cos(\\theta) = \\dfrac {A \\cdot B} {\\lVert A \\rVert \\lVert B \\rVert}$\n",
    "\n",
    "scipy에서의 cosine 함수는 $1 - cos(\\theta)$ 식(*dissimilarity*)를 사용한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "x = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], ndmin=2)\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0], ndmin=2)\n",
    "\n",
    "assert cosine(x, y) == 1 - np.sum(x * y) / (norm(x) * norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자카드 유사도\n",
    "\n",
    "$d_J(A, B) = 1 - J(A, B)$ $\\bigg (J(A, B) = \\dfrac {\\lvert A \\cap B \\rvert} {\\lvert A \\cup B \\rvert} \\bigg )$\n",
    "\n",
    "두 사용자와 아이템 사이의 합집합에 대한 교집합의 비율로 계산된다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import jaccard\n",
    "\n",
    "x = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], ndmin=2)\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0], ndmin=2)\n",
    "\n",
    "assert jaccard(x, y) == 1 - ((x == 1) & (y == 1)).sum() / ((x == 1) | (y == 1)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 피어슨 상관계수\n",
    "\n",
    "$r_{xy} = \\dfrac 1  {n - 1} \\displaystyle \\sum_{i=1}^n \\bigg (\\frac {x_i - \\bar x} {s_x} \\bigg ) \\bigg (\\frac {y_i - \\bar y} {s_y} \\bigg )$ <br>\n",
    "*($n$은 데이터의 총 차원 수를, $x_i, y_i$는 같은 차원에 매칭되는 벡터의 i번째 점이며 $\\bar x, \\bar y$는 $x, y$의 벡터를 나타낸다. $s_x, s_y$는 각각 벡터 $x, y$의 표준편차이다. )* <br>\n",
    "\n",
    "상관계수는 아래와 같이 두 변수의 공분산을 표준편차의 곱으로 나눠주는 방법으로 구할 수도 있다. <br>\n",
    "\n",
    "$\\rho_{X, Y} = \\dfrac {cov(X, Y)} {\\sigma_X \\sigma_Y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "x = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "y = np.array([0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0])\n",
    "\n",
    "assert pearsonr(x, y)[0] == (np.cov(x, y, bias=True))[0][1] / (np.std(x)*np.std(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬 인수 분해 \n",
    "\n",
    "Non-negative Matrix Factorization(NMF)를 이용하여 행렬 V를 행렬 W와 H의 곱으로 나타낸다.\n",
    "\n",
    "논문: http://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf <br>\n",
    "참고 사이트: http://vazic.me/non-negative-matrix-factorization-nmf/ <br>\n",
    "scikit-learn 코드 수정(*nan value에 대해서 작동하게*): https://github.com/scikit-learn/scikit-learn/pull/8474 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "toy code로 Nan에 대해서도 작동하는지 확인 <br>\n",
    "-> 기존 값들 정확하게 예측하고 nan에 해당되는 값들도 예측한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# 모델의 실행을 위해 toy data를 만든다. \n",
    "x = np.array([[5, 3, np.nan, 1], \n",
    "              [4, np.nan, np.nan, 1], \n",
    "              [1, 1, np.nan, 5], \n",
    "              [1, np.nan, np.nan, 4], \n",
    "              [np.nan, 1, 5, 4]])\n",
    "\n",
    "model = NMF(n_components=2, init='random', random_state=0, max_iter=1000, solver='mu')\n",
    "W = model.fit_transform(x)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0022716977250445171"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruction error를 check\n",
    "model.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.,   3.,  nan,   1.],\n",
       "       [  4.,  nan,  nan,   1.],\n",
       "       [  1.,   1.,  nan,   5.],\n",
       "       [  1.,  nan,  nan,   4.],\n",
       "       [ nan,   1.,   5.,   4.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.00001021,  2.99987694,  7.65165828,  1.00002874],\n",
       "       [ 3.99995931,  2.41647477,  6.29329943,  1.00001158],\n",
       "       [ 1.00030298,  0.99863188,  5.66008999,  5.00020773],\n",
       "       [ 0.99980858,  0.91531625,  4.79902998,  4.00004091],\n",
       "       [ 1.14799101,  1.0017324 ,  5.        ,  3.9996893 ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(W, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "사이즈가 좀 더 큰 데이터에 대해서 테스트 <br>\n",
    "-> toy code로 했을 때만큼 기존 데이터를 재현하지 못한다. 사이즈가 커서 그런건가? 어느 정도 수렴한 뒤에 더 이상 움직이지 않는건가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# dataset의 column명을 가져온다. \n",
    "data_cols = ['user id','movie id','rating','timestamp']\n",
    "item_cols = ['movie id','movie title','release date',\n",
    "'video release date','IMDb URL','unknown','Action',\n",
    "'Adventure','Animation','Childrens','Comedy','Crime',\n",
    "'Documentary','Drama','Fantasy','Film-Noir','Horror',\n",
    "'Musical','Mystery','Romance ','Sci-Fi','Thriller',\n",
    "'War' ,'Western']\n",
    "user_cols = ['user id','age','gender','occupation',\n",
    "'zip code']\n",
    "\n",
    "# ml-100k 데이터를 불러오자. \n",
    "users = pd.read_csv('./data/ml-100k/u.user', sep='|',\n",
    "names=user_cols, encoding='latin-1')\n",
    "item = pd.read_csv('./data/ml-100k/u.item', sep='|',\n",
    "names=item_cols, encoding='latin-1')\n",
    "data = pd.read_csv('./data/ml-100k/u.data', sep='\\t',\n",
    "names=data_cols, encoding='latin-1')\n",
    "\n",
    "# 불러온 데이터를 merge해서 하나의 dataframe으로 만들어준다. \n",
    "dataset = pd.merge(pd.merge(item, data),users)\n",
    "dataset = dataset.groupby(['user id', 'movie id'])[['rating']].mean().unstack().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NMF를 적용하자. \n",
    "model = NMF(n_components=2, init='random', random_state=0, max_iter=1000, solver='mu')\n",
    "W = model.fit_transform(dataset)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "273.77299581089511"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruction error를 check\n",
    "model.reconstruction_err_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.87402152,  3.04713887,  2.99439593, ...,  1.5847179 ,\n",
       "         4.22659688,  3.32604187],\n",
       "       [ 3.93575325,  3.16648342,  3.07518656, ...,  1.72123861,\n",
       "         3.98283495,  3.48588179],\n",
       "       [ 3.30014078,  2.71048621,  2.60442909, ...,  1.53031256,\n",
       "         3.09622636,  3.0065066 ],\n",
       "       ..., \n",
       "       [ 4.31366119,  3.53881755,  3.40237188, ...,  1.99385249,\n",
       "         4.06513087,  3.92366317],\n",
       "       [ 4.5230543 ,  3.94970497,  3.6792498 , ...,  2.46647368,\n",
       "         3.21160582,  4.47500288],\n",
       "       [ 3.88703535,  3.26798735,  3.10286165, ...,  1.92108897,\n",
       "         3.31517116,  3.65508697]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W*H가 기존 데이터를 재현하는지 확인\n",
    "np.dot(W, H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user id\n",
       "1      5.0\n",
       "2      4.0\n",
       "3      NaN\n",
       "4      NaN\n",
       "5      4.0\n",
       "6      4.0\n",
       "7      NaN\n",
       "8      NaN\n",
       "9      NaN\n",
       "10     4.0\n",
       "11     NaN\n",
       "12     NaN\n",
       "13     3.0\n",
       "14     NaN\n",
       "15     1.0\n",
       "16     5.0\n",
       "17     4.0\n",
       "18     5.0\n",
       "19     NaN\n",
       "20     3.0\n",
       "21     5.0\n",
       "22     NaN\n",
       "23     5.0\n",
       "24     NaN\n",
       "25     5.0\n",
       "26     3.0\n",
       "27     NaN\n",
       "28     NaN\n",
       "29     NaN\n",
       "30     NaN\n",
       "      ... \n",
       "914    NaN\n",
       "915    NaN\n",
       "916    4.0\n",
       "917    3.0\n",
       "918    3.0\n",
       "919    4.0\n",
       "920    NaN\n",
       "921    3.0\n",
       "922    5.0\n",
       "923    3.0\n",
       "924    5.0\n",
       "925    NaN\n",
       "926    NaN\n",
       "927    5.0\n",
       "928    NaN\n",
       "929    3.0\n",
       "930    3.0\n",
       "931    NaN\n",
       "932    4.0\n",
       "933    3.0\n",
       "934    2.0\n",
       "935    3.0\n",
       "936    4.0\n",
       "937    NaN\n",
       "938    4.0\n",
       "939    NaN\n",
       "940    NaN\n",
       "941    5.0\n",
       "942    NaN\n",
       "943    NaN\n",
       "Name: (rating, 1), Length: 943, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교대 최소 제곱\n",
    "\n",
    "`implicit` 이라는 패키지를 사용해서 교대 최소 제곱을 구해보자. 아래 블로그에 구현하는 예제가 있으니 보도록 하자.\n",
    "\n",
    "참고 사이트: \n",
    "\n",
    "1. scratch: https://bugra.github.io/work/notes/2014-04-19/alternating-least-squares-method-for-collaborative-filtering/\n",
    "2. package: https://github.com/benfred/implicit\n",
    "3. example: http://www.benfrederickson.com/matrix-factorization/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "참고 사이트: \n",
    "\n",
    "1) SVD 설명: http://darkpgmr.tistory.com/106"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FULL ---\n",
      "U:\n",
      " [[-0.138  0.024  0.011  0.99  -0.    -0.     0.   ]\n",
      " [-0.413  0.071  0.032 -0.059 -0.885  0.192  0.   ]\n",
      " [-0.55   0.094  0.043 -0.079  0.424  0.707  0.   ]\n",
      " [-0.688  0.118  0.054 -0.099  0.192 -0.681  0.   ]\n",
      " [-0.153 -0.591 -0.654 -0.     0.    -0.    -0.447]\n",
      " [-0.072 -0.731  0.678  0.    -0.     0.     0.   ]\n",
      " [-0.076 -0.296 -0.327 -0.    -0.    -0.     0.894]]\n",
      "s:\n",
      " [ 12.481   9.509   1.346   0.      0.   ]\n",
      "VT:\n",
      " [[-0.562 -0.593 -0.562 -0.09  -0.09 ]\n",
      " [ 0.127 -0.029  0.127 -0.695 -0.695]\n",
      " [ 0.41  -0.805  0.41   0.091  0.091]\n",
      " [-0.707  0.     0.707 -0.     0.   ]\n",
      " [-0.     0.    -0.     0.707 -0.707]]\n",
      "--- REDUCED ---\n",
      "U:\n",
      " [[-0.138  0.024  0.011  0.99  -0.   ]\n",
      " [-0.413  0.071  0.032 -0.059 -0.885]\n",
      " [-0.55   0.094  0.043 -0.079  0.424]\n",
      " [-0.688  0.118  0.054 -0.099  0.192]\n",
      " [-0.153 -0.591 -0.654 -0.     0.   ]\n",
      " [-0.072 -0.731  0.678  0.    -0.   ]\n",
      " [-0.076 -0.296 -0.327 -0.    -0.   ]]\n",
      "s:\n",
      " [ 12.481   9.509   1.346   0.      0.   ]\n",
      "VT:\n",
      " [[-0.562 -0.593 -0.562 -0.09  -0.09 ]\n",
      " [ 0.127 -0.029  0.127 -0.695 -0.695]\n",
      " [ 0.41  -0.805  0.41   0.091  0.091]\n",
      " [-0.707  0.     0.707 -0.     0.   ]\n",
      " [-0.     0.    -0.     0.707 -0.707]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1, 1, 1, 0, 0],\n",
    "              [3, 3, 3, 0, 0],\n",
    "              [4, 4, 4, 0, 0],\n",
    "              [5, 5, 5, 0, 0],\n",
    "              [0, 2, 0, 4, 4],\n",
    "              [0, 0, 0, 5, 5],\n",
    "              [0, 1, 0, 2, 2]])\n",
    "\n",
    "# set numpy printing options\n",
    "np.set_printoptions(suppress=True)\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "# Full SVD is taught more often. Here is a good explination of the different\n",
    "# http://www.cs.cornell.edu/Courses/cs322/2008sp/stuff/TrefethenBau_Lec4_SVD.pdf\n",
    "print(\"--- FULL ---\")\n",
    "U, s, VT = np.linalg.svd(a, full_matrices=True)\n",
    "\n",
    "print(\"U:\\n {}\".format(U))\n",
    "print(\"s:\\n {}\".format(s))\n",
    "print(\"VT:\\n {}\".format(VT))\n",
    "\n",
    "# the reduced or trucated SVD operation can save time by ignoring all the\n",
    "# extremly small or exactly zero values. A good blog post explaing the benefits\n",
    "# can be found here:\n",
    "# http://blog.explainmydata.com/2016/01/how-much-faster-is-truncated-svd.html\n",
    "print(\"--- REDUCED ---\")\n",
    "\n",
    "U, s, VT = np.linalg.svd(a, full_matrices=False)\n",
    "\n",
    "print(\"U:\\n {}\".format(U))\n",
    "print(\"s:\\n {}\".format(s))\n",
    "print(\"VT:\\n {}\".format(VT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping aggregation(Bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM, Linear regression, Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge vs Lasso \n",
    "\n",
    "출처: https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
