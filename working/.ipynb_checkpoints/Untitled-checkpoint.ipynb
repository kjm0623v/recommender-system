{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 post에서는 새로운 두 가지 패키지를 사용해서 implicit matrix factorization의 WARP를 구현할 예정이다. WARP는 학습할 때 두 item 의 rank에 대해서 고려하기 때문에 ***Learning to Rank*** 라고 한다.\n",
    "\n",
    "### 패키지\n",
    "\n",
    "1. [LightFM](http://lyst.github.io/lightfm/docs/home.html): BPR, WARP 등 여러가지 implicit MF 모델이 구현되어 있다. 이 중 WARP를 택해서 학습시키고 결과를 확인한다. Cython으로 작성되었고 HOGWILD SGD로 병렬처리가 되어 있어서 학습이 매우 빠르다. \n",
    "    - 설치 방법: pip install lightfm <br><br>\n",
    "2. [scikit-optimize](https://scikit-optimize.github.io/): optimization에 사용되는 패키지로 좀 더 빠르게 모델에 적합한 hyperparameter를 찾기 위해 사용한다.\n",
    "    - 설치 방법: pip install scikit-optimize\n",
    "    \n",
    "### 모델 소개\n",
    "\n",
    "앞에서 다룬 내용에서는 데이터셋 하나씩만을 가지고(*explicit feedback, implicit feedback*) 하나의 모델을 학습시키는 형식으로 진행되었다.\n",
    "이번에는 implicit feedback과 item에 관련된 feature, 두 가지 데이터를 사용해서 모델을 학습시켜보고 그렇지 않을 때와 성능을 비교해보도록 하자.\n",
    "\n",
    "이와 관련된 개념은 [Matrix Factorization Techniques for Recommender Systems](https://endymecy.gitbooks.io/spark-ml-source-analysis/content/%E6%8E%A8%E8%8D%90/papers/Matrix%20Factorization%20Techniques%20for%20Recommender%20Systems.pdf)의 소제목 \"Additional Input Sources\" 내용에 언급되어 있다. side-information을 모델의 학습을 돕기 위해 넣어주는 것인데 예를 들어 아래와 같은 user의 프로필이 있다고 하자. \n",
    "\n",
    "| Feature        | Value           |\n",
    "|:-------------|:-------------|\n",
    "| gender      | Female |\n",
    "| age_bucket      | 25-34      |\n",
    "| income_bucket | $65-79K      |\n",
    "\n",
    "우린 이 정보를 one-hot-encode할 수 있고 각각의 feature들을 $A$라는 \"attribute-space\"에 존재할 것이라고 가정할 수 있다. 그리고 여기에 속한 각각의 attribute인 $a$는 user vector인 $\\textbf{x}_u$와 같이 latent vector $\\textbf{s}_a$를 가진다고 가정하자. 이제 side-information을 기존 user vector에 반영해주면 아래와 같이 user vector를 정의할 수 있다.\n",
    "\n",
    "$\\textbf{x}_u + \\displaystyle \\sum_{a \\in N(u)} \\textbf{s}_a$\n",
    "\n",
    "접근 방법은 간단한데 이걸 기존에 구현했던 방법들에 적용할 경우에 연산량 문제로 인해서 ALS나 SGD에 사용하기 어렵다고 한다. (*왜 그런지는 더 확인 필요.*)\n",
    "\n",
    "그래서 두 가지 다른 방법인 **BPR**와 **WARP**를 소개한다. 아래에서 차례대로 살펴보자.\n",
    "\n",
    "### Learning to Rank - BPR\n",
    "\n",
    "It turns out that there is another method of optimizing implicit feedback matrix factorization problems which is neither ALS nor conventional stochastic gradient descent (SGD) on last post's objective function. This method of optimization typically goes by the name [Learning to Rank](https://en.wikipedia.org/wiki/Learning_to_rank) and originated in information retrieval theory.\n",
    "\n",
    "A classic method of using Learning to Rank with implicit feedback was in the paper [BPR: Bayesian Personalized Ranking from Implicit Feedback](https://arxiv.org/pdf/1205.2618.pdf) (pdf link) first-authored by Steffen Rendle who is kind of a badass in all things implicit and factorized. The idea is centered around sampling *positive* and *negative* items and running pairwise comparisons. Let's assume that for this example our dataset consists of the number of times users have clicked on various items on a website. BPR proceeds as follows (in simplified form):\n",
    "\n",
    "1. Randomly select a user $u$ and then select a random item $i$ which the user has clicked on. This is our *positive* item.\n",
    "2. Randomly select an item $j$ which the user has clicked on $fewer$ times than item $i$ (this includes items that they have never clicked). This is our *negative* item.\n",
    "3. Use whatever equation you want to predict the \"score\", $p_{ui}$, for user $u$ and positive item $i$. For matrix factorization, this may be $\\textbf{x}_{u} \\cdot \\textbf{y}_{i}$.\n",
    "4. Predict the score for user $u$ and negative item $j$, $p_{uj}$.\n",
    "5. Find the difference between the positive and negative scores $x_{uij} = p_{ui} - p_{uj}$.\n",
    "6. Pass this difference through a sigmoid and use it as a weighting for updating all of the model parameters via stochastic gradient descent (SGD).\n",
    "\n",
    "This method seemed kind of radical to me when I first read it. Notably, we do not care about the actual value of the score that we are predicting. All we care about is that we rank items which the user has clicked on more frequently higher than items which the user has clicked on fewer times. And thus, our model \"learns to rank\" :)\n",
    "\n",
    "Because this model employs a sampling-based approach, the authors show that it can be quite fast and scalable relative to other, slower methods. Additionally, the authors argue that BPR directly optimizes the area under the ROC curve (AUC) which could be a desirable characteristic. Most importantly for me, it allows one to easily add in side information without blowing up the computational speed.\n",
    "\n",
    "## Learning to Rank - WARP\n",
    "\n",
    "WARP는 BPR과 비슷하지만 약간 다른 방식을 취한다. user를 정해놓고 positive, negative item을 sampling한다. 그리고 이 둘에 대해서 우리의 모델로 score를 예측을 한다. 이 때, negative item이 positive item보다 score가 높다고 *잘못 예측한 경우*에 대해서만 SGD update를 한다. *잘 예측한 경우* 에는 계속 sampling을 진행하고 전체 item에 대해서 sampling을 해도 되지만 연산량을 고려해서 $N$개를 cutoff value로 지정해놓고 이를 넘을 경우에는 다음 user로 넘어가는 방식이다.\n",
    "\n",
    "WARP에서 추가되는 hyperparameter는 위에서 소개한 **cutoff value** 외에 한 가지가 더 있다. 바로 **margin** 인데 negative item이 positive item보다 score가 얼마나 높을 때를 기준으로 삼느냐를 정하기 위해 사용한다. 논문에선 $p_{uj} > p_{ui} + 1$ 식과 같이 $1$을 margin으로 두었다.\n",
    "\n",
    "sampling 형식으로 학습이 진행되므로 초반에는 *잘못 예측하는 경우* 가 많아서 학습이 빠르게 되지만 시간이 지날 수록 *잘 예측해서* 학습이 느려질 것이라고 예상할 수 있다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정리 필요 사항\n",
    "\n",
    "1. LightFM 패키지에서 side-information을 어떻게 다루는지: [github](https://github.com/lyst/lightfm/blob/cc43bedbc995594eaf1aba11da4dde4ddb8a8f2a/lightfm/lightfm.py#L124) 소스 코드에 feature matrix를 입력할 때의 내용과 필요 시 identity matrix를 concat하는 방법을 고려하라고 되어 있는데 이 내용이 어떻게 수학적으로 처리가 되는지 모르겠다.\n",
    "2. 학습이 완료되었을 때 item_embedding이 포함하는 값이 tag + item 둘 다인가? 그래서 item/tag 만의 vector를 얻기 위해서 내적을 해줘야 하나? \n",
    "    - tag를 고른 뒤 유사 item을 예측할 때: \n",
    "        - tag vector: item_embeddings에서 tag idx에 해당하는 값 \n",
    "        - item matrix: item_features_concat과 item_embeddings를 내적한 값\n",
    "    - item을 고른 뒤 유사 tag를 예측할 때: \n",
    "        - item vector: item_features_concat에서 item idx에 해당하는 값과 item_embeddings를 내적한 값\n",
    "        - tag matrix: item_embeddings 값 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
