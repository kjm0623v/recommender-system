{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 내용은 Ethan Rosenthal의 블로그의 내용과 소스코드를 바탕으로 번역 및 일부 수정한 내용입니다.\n",
    "\n",
    "원본을 보시고 싶으신 분은 아래 사이트를 방문해주세요\n",
    "\n",
    "- 블로그: http://blog.ethanrosenthal.com/\n",
    "- github: https://github.com/EthanRosenthal/DataPiques_source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞으로 몇 번에 걸쳐서 collaborative filtering에 속해있는 Matrix Factorization(MF)을 살펴볼 예정입니다. 이 페이지에서는 MF의 목적 함수를 정의하고 이를 학습하는 대표적인 방법 2가지 ALS, SGD의 알고리즘을 살펴보고 구현 할 것입니다.\n",
    "\n",
    "### Matrix factorization 가정 \n",
    "- 각 user는 k개의 feature로 이루어진 vector로 표현될 수 있으며 각 feature는 user가 SF 영화를 얼마나 좋아하는 지 등의 특징을 나타낸다. (user의 item에 대한 선호도)\n",
    "- 각 item은 k개의 feature로 이루어진 vector로 표현될 수 있으며 각 feature는 item이 얼마나 SF 영화에 가까운 장르인지를 나타낸다. \n",
    "(item의 특징)\n",
    "- user vector와 item vector의 각각 매칭되는 feature들을 곱한 뒤 모두 더하면(*내적*) 이는 user가 item에 대한 예상 평점이라고 볼 수 있다.\n",
    "\n",
    "위 가정에서 좋은 점은 우리가 k개의 feature 개수만 정해주고 아무런 개입없이 학습만 시켜주면 해당되는 값을 얻을 수 있다는 점이다. <br>\n",
    "그럼 이제, 학습을 위한 목적 함수를 만들어보자.\n",
    "\n",
    "### 목적 함수 \n",
    "\n",
    "먼저 함수에 사용할 항들의 notation을 짚고 넘어가자. $k$는 feature의 개수, $\\textbf{x}_{u}$는 u번째 user의 vector, $\\textbf{y}_{i}$는 i번째 item의 vector이다. 위에서 가정했듯, $\\textbf{x}_{u}$, $\\textbf{y}_{i}$을 내적하면 예상 평점을 구할 수 있다. 아래와 같이 $\\hat r_{ui}$를 우리가 얻은 vector로부터 구한 예상 평점이라고 하자.\n",
    "\n",
    "$$\\hat r_{ui} = \\textbf{x}_{u}^{\\intercal} \\cdot{} \\textbf{y}_{i} = \\sum\\limits_{k} x_{uk}y_{ki}$$\n",
    "\n",
    "자, 그럼 학습 시킬 때의 진짜 평점도 있을 것이다. 이는 $r_{ui}$로 두자. 이제 목적 함수를 만들기 위한 준비가 끝났다.\n",
    "\n",
    "Matrix factorization을 통해 궁극적으로 원하는 내용은 ***우리가 나타낸 k개의 feature로 이루어진 user, item으로부터 예상 평점을 구했을 때 진짜 평점과 완전히 같은 것.*** 이다. 이를 식으로 나타내면 아래와 같으며 $L$을 최소화 시켰을 때 우리가 원하는 $\\textbf{x}_{u}$, $\\textbf{y}_{i}$을 구할 수 있다. 뒤 항들은 생소할 수 있는데 overfitting을 막기 위해 $L_{2}$ regularization term을 적용한 것이다.\n",
    "\n",
    "$$L = \\sum\\limits_{u,i \\in S}(r_{ui} - \\textbf{x}_{u}^{\\intercal} \\cdot{} \\textbf{y}_{i})^{2} + \\lambda_{x} \\sum\\limits_{u} \\left\\Vert \\textbf{x}_{u} \\right\\Vert^{2} + \\lambda_{y} \\sum\\limits_{u} \\left\\Vert \\textbf{y}_{i} \\right\\Vert^{2}$$\n",
    "\n",
    "목적 함수를 정의했으니 이를 어떻게 학습할 건지에 대한 방법을 정해야 한다. 위에서 소개했듯 ALS, SGD를 차례로 보도록 하겠다.\n",
    "\n",
    "### Alternaing Least Squares (ALS)\n",
    "\n",
    "ALS 방법은 지금 우리가 학습시켜야 할 항이 2개 ($\\textbf{x}_{u}, \\textbf{y}_{i}$)가 있는데 $\\textbf{x}_{u}$를 학습할 때는 $\\textbf{y}_{i}$를 상수로 두고, 반대로 $\\textbf{y}_{i}$를 학습할 때는 $\\textbf{x}_{u}$를 상수로 두고 학습한다.\n",
    "\n",
    "수식을 통해 보면 좀 더 이해가 잘 된다. 우리가 원하는 건 $L$이 최소값일 때의 $\\textbf{x}_{u}$를 구하는 것이므로 $\\dfrac{\\partial L}{\\partial \\textbf{x}_{u}} = 0$ 을 만족하는 $\\textbf{x}_{u}$ 값을 구해주면 된다. 이게 성립하는 이유는 $\\textbf{y}_{i}$ 항을 \n",
    "상수로 뒀기 때문에 $\\textbf{x}_{u}$에 대한 식만 풀면 되고 *convex* 해서 그냥 등식을 풀어주면 되기 때문이다. \n",
    "\n",
    "그래서 최종적으로 $\\textbf{x}_{u}^{\\intercal} = \\textbf{r}_{u}Y(Y^{\\intercal}Y + \\lambda_{x}I)^{-1}$ 값으로 update를 해주면 된다.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\textbf{x}_{u}} = - 2 \\sum\\limits_{i}(r_{ui} - \\textbf{x}_{u}^{\\intercal} \\cdot{} \\textbf{y}_{i}) \\textbf{y}_{i}^{\\intercal} + 2 \\lambda_{x} \\textbf{x}_{u}^{\\intercal}$$\n",
    "\n",
    "$$0 = -(\\textbf{r}_{u} - \\textbf{x}_{u}^{\\intercal} Y^{\\intercal})Y + \\lambda_{x} \\textbf{x}_{u}^{\\intercal}$$\n",
    "\n",
    "$$\\textbf{x}_{u}^{\\intercal}(Y^{\\intercal}Y + \\lambda_{x}I) = \\textbf{r}_{u}Y$$\n",
    "\n",
    "$$\\textbf{x}_{u}^{\\intercal} = \\textbf{r}_{u}Y(Y^{\\intercal}Y + \\lambda_{x}I)^{-1}$$\n",
    "\n",
    "$\\textbf{y}_{i}$에 대해서도 마찬가지이다. 위와 같이 수식을 풀어서 쓰면 아래 등식을 푸는 과정이 되고 최종적으로 \n",
    "$ \\textbf{y}_{i}^{\\intercal} =  \\textbf{r}_{i} X  ( X^{\\intercal}X +  \\lambda_{y}I) ^{-1}$ 값을 update 해주면 된다.\n",
    "\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\textbf{y}_{i}} = - 2 \\sum\\limits_{i}(r_{iu} - \\textbf{y}_{i}^{\\intercal} \\cdot{} \\textbf{x}_{u}) \\textbf{x}_{u}^{\\intercal} + 2 \\lambda_{y} \\textbf{y}_{i}^{\\intercal}$$\n",
    "\n",
    "$$0 = -(\\textbf{r}_{i} - \\textbf{y}_{i}^{\\intercal} X^{\\intercal})X + \\lambda_{y} \\textbf{y}_{i}^{\\intercal}$$\n",
    "\n",
    "$$ \\textbf{y}_{i}^{\\intercal} ( X^{\\intercal}X +  \\lambda_{y}I) =  \\textbf{r}_{i} X$$\n",
    "\n",
    "$$ \\textbf{y}_{i}^{\\intercal} =  \\textbf{r}_{i} X  ( X^{\\intercal}X +  \\lambda_{y}I) ^{-1}$$\n",
    "\n",
    "#### ALS 내용을 정리해보자. <br>\n",
    "1. user, item을 학습시켜야 하는데 한 쪽을 학습시킬 때는 다른 한 쪽은 상수로 취급한다.\n",
    "2. 한 쪽이 상수로 취급되면 목적함수가 최소일 때의 user(혹은 item) vector 값을 등식 계산을 통해 얻을 수 있다.\n",
    "3. 2번 과정을 계속 반복하면 최적의 user, item vector를 구할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 구현\n",
    "\n",
    "아래 코드에서는 위에서 언급한 목적 함수를 구현하고, ALS 방법을 통해서 user, item의 vector를 학습시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "SGD에서도 위 ALS에서 사용한 목적 함수를 그대로 사용하며 학습 방법만 약간의 차이가 있다. ALS에서는 user를 update할 때는 item을 상수로 두고 등식을 풀었는데 SGD에서는 두 항에 대해서 각각 gradient를 구한 뒤 현재 값에 일부 update를 시켜서 수렴할 때까지 update를 하는 방식을 취한다.\n",
    "\n",
    "SGD를 다룰 떄 추가적인 항들을 넣어줄 것인데 바로 ***bias***이다. user 중에는 남들보다 조금 더 비판적인 사람이 있을 수 있고 관대한 사람들도 있을 수 있다. item도 지금 상황에 특정 장르가 인기가 많을 수도 있고 적을 수도 있다. 이런 특징들을 반영해주는 것이 bias이다. \n",
    "\n",
    "bias를 추가해서 예상 평점을 식으로 나타내면 아래와 같다. 여기서 $\\mu$는 overall average rating으로 전체적인 rating의 평균을 나타내서 평점이 이를 기준으로 bias를 고려해서 위 아래로 나타나도록 하는 역할을 한다. $$ \\hat r_{ui} = \\mu + b_{u} + b_{i} + \\textbf{x}_{u}^{\\intercal} \\cdot{} \\textbf{y}_{i} $$ \n",
    "\n",
    "목적 함수를 다시 정의하면 아래 식이 된다. \n",
    "$$L = \\sum\\limits_{u,i}(r_{ui} - (\\mu + b_{u} + b_{i} + \\textbf{x}_{u}^{\\intercal} \\cdot{} \\textbf{y}_{i}))^{2} + \n",
    "\\lambda_{xb} \\sum\\limits_{u} \\left\\Vert b_{u} \\right\\Vert^{2} + \\lambda_{yb} \\sum\\limits_{i} \\left\\Vert b_{i} \\right\\Vert^{2} + \\lambda_{xf} \\sum\\limits_{u} \\left\\Vert \\textbf{x}_{u} \\right\\Vert^{2} + \\lambda_{yf} \\sum\\limits_{u} \\left\\Vert \\textbf{y}_{i} \\right\\Vert^{2}$$\n",
    "\n",
    "각 항 별로, gradient를 구해서 $\\eta$(learning rate) 만큼 update를 시켜줘야 한다. 예시로 $b_u$에 대해서만 구해보도록 하자.\n",
    "아래 식에서 $e_{ui}$는 예측에 대한 error를 나타낸다. \n",
    "\n",
    "$$ b_{u} \\leftarrow b_{u} - \\eta \\frac{\\partial L}{\\partial b_{u}} $$\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial b_{u}} = 2(r_{ui} - (\\mu + b_{u} + b_{i} + \\textbf{x}_{u}^{\\intercal} \\cdot{} \\textbf{y}_{i}))(-1) + 2\\lambda_{xb} b_{u} $$\n",
    "$$ \\frac{\\partial L}{\\partial b_{u}} = 2(e_{ui})(-1) + 2\\lambda_{xb} b_{u} $$\n",
    "$$ \\frac{\\partial L}{\\partial b_{u}} = - e_{ui} + \\lambda_{xb} b_{u} $$\n",
    "\n",
    "위와 같은 전개를 통해서 gradient를 구했고 구한 gradient를 기존 항에 update를 시켜주면 된다. 나머지 항들에 대해서도 같은 방법으로 적용이 되며 최종적으로 아래 4개 항에 대해서 동시에 update가 진행된다. \n",
    "\n",
    "$$ b_{u} \\leftarrow b_{u} + \\eta \\, (e_{ui} - \\lambda_{xb} b_{u}) $$\n",
    "$$ b_{i} \\leftarrow b_{i} + \\eta \\, (e_{ui} - \\lambda_{yb} b_{i}) $$\n",
    "$$ \\textbf{x}_{u} \\leftarrow \\textbf{x}_{u} + \\eta \\, (e_{ui}\\textbf{y}_{i} - \\lambda_{xf} \\textbf{x}_{u}) $$\n",
    "$$ \\textbf{y}_{i} \\leftarrow \\textbf{y}_{i} + \\eta \\, (e_{ui}\\textbf{x}_{u} - \\lambda_{yf} \\textbf{y}_{i}) $$\n",
    "\n",
    "#### SGD 내용을 정리해보자.\n",
    "1. user, item에 대한 bias를 각각 추가한다. regularization term에도 고려한다.\n",
    "2. user, item, bias를 구하는 것이 최종 목적이며 ALS와 다르게 어떤 항도 상수로 두지 않는다.\n",
    "3. 항이 여러 개이므로 최적의 값을 바로 구할 수 없고 항 별로 현재 상태에서 $L$에 대한 gradient를 구한 뒤 이를 update해서 $L$이 최소가 되는 값으로 나아가는 방법을 써야한다. (Gradient Descent)\n",
    "4. 3번 과정을 계속 반복하면 최적은 user, item, bias를 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
